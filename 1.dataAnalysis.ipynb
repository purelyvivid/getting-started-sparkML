{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始spark設定(每台不同)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['SPARK_HOME'] = '/opt/cloudera/parcels/CDH/lib/spark/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 欄位資訊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"col_info.txt\", \"r\") as f:\n",
    "    head = f.readline()\n",
    "    col_info = [ line.strip().split(\" \") for line in f.read().split(\"\\n\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0', 'Year', 'int', '飛行日期_年', 'X'],\n",
       " ['1', 'Month', 'str', '飛行日期_月', 'O', '一月和十二月特別容易取消班機'],\n",
       " ['2', 'DayofMonth', 'int', '飛行日期_日', 'O', '11號和13號', '特別容易取消班機'],\n",
       " ['3', 'DayOfWeek', 'int', '飛行日期_星期', 'O', '星期二特別容易取消班機,星期日最不容易取消班機'],\n",
       " ['4', 'DepTime', 'str', '飛行時間_起飛時間', 'X', \"如果班機取消,這個值必為'NA',不可選\"]]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_info[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 讀取資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('yarn')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(ci[1], IntegerType() if ci[2]=='int' else StringType() , False) for ci in col_info ])\n",
    "\n",
    "inspections = sqlContext.read.format('com.databricks.spark.csv') \\\n",
    "        .options(header='true', inferschema='true') \\\n",
    "        .schema(schema) \\\n",
    "        .load('2000.csv')\n",
    "\n",
    "inspections.createOrReplaceTempView(\"inspections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Year=2000, Month='1', DayofMonth=28, DayOfWeek=5, DepTime='1647', CRSDepTime='1647', ArrTime='1906', CRSArrTime='1859', UniqueCarrier='HP', FlightNum='154', TailNum='N808AW', ActualElapsedTime='259', CRSElapsedTime='252', AirTime='233', ArrDelay=7, DepDelay=0, Origin='ATL', Dest='PHX', Distance=1587, TaxiIn='15', TaxiOut='11', Cancelled=0, CancellationCode='NA', Diverted='0', CarrierDelay='NA', WeatherDelay='NA', NASDelay='NA', SecurityDelay='NA', LateAircraftDelay='NA')]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspections.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import UserDefinedFunction, count, countDistinct, round, concat, length, sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_text = \"\"\"select X.{colname} , X.C as Qty, round(X.C/Y.C*100,2) as Ratio\n",
    "              from (select {colname}, count({colname}) as C from inspections where Cancelled=1 group by {colname}) X \n",
    "              join (select {colname}, count({colname}) as C from inspections group by {colname}) Y on X.{colname}=Y.{colname}\n",
    "            order by Ratio DESC\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+-----+\n",
      "|UniqueCarrier|  Qty|Ratio|\n",
      "+-------------+-----+-----+\n",
      "|           UA|44159| 5.69|\n",
      "|           AS| 7506| 4.87|\n",
      "|           HP| 9422|  4.3|\n",
      "|           AA|29677|  4.0|\n",
      "|           US|28055| 3.75|\n",
      "|           DL|31569| 3.48|\n",
      "|           NW|15340| 2.78|\n",
      "|           TW| 5254| 1.97|\n",
      "|           CO| 7296| 1.86|\n",
      "|           AQ|  173| 1.57|\n",
      "|           WN| 9039| 0.99|\n",
      "+-------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(sql_text.format(colname=\"UniqueCarrier\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+\n",
      "|Origin|  Qty|Ratio|\n",
      "+------+-----+-----+\n",
      "|   DUT|  166|25.08|\n",
      "|   PIA|    3|16.67|\n",
      "|   MRY|   51|14.96|\n",
      "|   HPN|  604|12.07|\n",
      "|   SCC|   42|10.99|\n",
      "|   BRW|  105|10.95|\n",
      "|   DLG|   32| 10.6|\n",
      "|   AKN|   32| 9.88|\n",
      "|   BET|   92| 8.88|\n",
      "|   ADQ|   58| 7.97|\n",
      "|   LGA| 7591| 7.29|\n",
      "|   BOS| 7689|  6.8|\n",
      "|   CID|  342| 6.65|\n",
      "|   ORD|19318| 6.51|\n",
      "|   TRI|   70| 6.08|\n",
      "|   TOL|   54| 5.99|\n",
      "|   OME|   56| 5.96|\n",
      "|   BQN|    1| 5.88|\n",
      "|   RST|  131| 5.86|\n",
      "|   SWF|   74| 5.79|\n",
      "+------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(sql_text.format(colname=\"Origin\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|Dest|  Qty|Ratio|\n",
      "+----+-----+-----+\n",
      "| DUT|  164|24.77|\n",
      "| MRY|   49|14.37|\n",
      "| HPN|  579|11.58|\n",
      "| SCC|   41|10.73|\n",
      "| BRW|   96|10.01|\n",
      "| DLG|   30| 9.93|\n",
      "| AKN|   31| 9.57|\n",
      "| BET|   91| 8.78|\n",
      "| ADQ|   58| 7.97|\n",
      "| TRI|   83|  7.2|\n",
      "| LGA| 7287|  7.0|\n",
      "| ORD|20409| 6.87|\n",
      "| BOS| 7502| 6.63|\n",
      "| PIA|    1| 6.25|\n",
      "| CID|  320| 6.22|\n",
      "| TOL|   55|  6.1|\n",
      "| OME|   56| 5.96|\n",
      "| ANC| 1222| 5.88|\n",
      "| SWF|   73| 5.71|\n",
      "| BQN|    1| 5.56|\n",
      "+----+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(sql_text.format(colname=\"Dest\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+\n",
      "|DayofMonth| Qty|Ratio|\n",
      "+----------+----+-----+\n",
      "|        25|9010| 4.88|\n",
      "|        11|9014| 4.81|\n",
      "|        18|8961| 4.78|\n",
      "|        13|8383| 4.45|\n",
      "|        31|4695| 4.28|\n",
      "|        30|7036| 4.14|\n",
      "|        14|7708| 4.07|\n",
      "|        20|7294| 3.87|\n",
      "|        19|7070| 3.78|\n",
      "|        12|6965| 3.73|\n",
      "|        17|6953| 3.68|\n",
      "|        10|6600|  3.5|\n",
      "|        16|6362| 3.42|\n",
      "|        26|6323| 3.39|\n",
      "|        28|6079| 3.24|\n",
      "|        15|5981| 3.22|\n",
      "|        21|5886| 3.11|\n",
      "|        27|5467| 2.91|\n",
      "|        29|5307| 2.87|\n",
      "|         2|5248| 2.86|\n",
      "+----------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(sql_text.format(colname=\"DayofMonth\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_text = \"\"\"select X.{colname} , X.C as Qty, round(X.C/Y.C*100,2) as Ratio\n",
    "              from (select {colname}, count({colname}) as C from inspections where Cancelled=1 group by {colname}) X \n",
    "              join (select {colname}, count({colname}) as C from inspections group by {colname}) Y on X.{colname}=Y.{colname}\n",
    "            order by int(X.{colname})\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|Month|  Qty|Ratio|\n",
      "+-----+-----+-----+\n",
      "|    1|24515| 5.21|\n",
      "|    2|15188| 3.42|\n",
      "|    3|10237| 2.12|\n",
      "|    4|11642| 2.51|\n",
      "|    5|16513| 3.45|\n",
      "|    6|18632| 3.95|\n",
      "|    7|15526| 3.21|\n",
      "|    8|14991| 3.05|\n",
      "|    9|10365| 2.24|\n",
      "|   10|10369| 2.13|\n",
      "|   11|10912| 2.34|\n",
      "|   12|28600| 5.95|\n",
      "+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(sql_text.format(colname=\"Month\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+-----+\n",
      "|DayofWeek|  Qty|Ratio|\n",
      "+---------+-----+-----+\n",
      "|        1|27812| 3.37|\n",
      "|        2|31724| 3.83|\n",
      "|        3|29199| 3.51|\n",
      "|        4|27804| 3.36|\n",
      "|        5|27173| 3.28|\n",
      "|        6|23137| 3.12|\n",
      "|        7|20641| 2.58|\n",
      "+---------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(sql_text.format(colname=\"DayofWeek\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_text = \"\"\"select X.HR , X.C as Qty, round(X.C/Y.C*100,2) as Ratio\n",
    "              from (select HR, count(HR) as C from (select int({colname}/100) as HR from inspections where Cancelled=1) group by HR) X \n",
    "              join (select HR, count(HR) as C from (select int({colname}/100) as HR from inspections) group by HR) Y on X.HR=Y.HR\n",
    "            order by Ratio DESC\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+\n",
      "| HR|  Qty|Ratio|\n",
      "+---+-----+-----+\n",
      "| 19|12364| 3.98|\n",
      "| 17|15933| 3.96|\n",
      "| 16|11375| 3.58|\n",
      "| 15|12609| 3.55|\n",
      "| 18|12136| 3.54|\n",
      "| 20|10197|  3.4|\n",
      "|  0|  728| 3.34|\n",
      "| 14|10754| 3.34|\n",
      "| 13|12282| 3.32|\n",
      "|  5|  799| 3.26|\n",
      "| 10|10902| 3.26|\n",
      "| 11|11133| 3.18|\n",
      "| 21| 5459| 3.16|\n",
      "|  6|11187| 3.06|\n",
      "| 12|10759| 3.06|\n",
      "|  7|12214| 2.99|\n",
      "|  9|10544| 2.92|\n",
      "|  8|11345|  2.9|\n",
      "|  2|   34| 2.88|\n",
      "| 22| 3505| 2.77|\n",
      "+---+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(sql_text.format(colname=\"CRSDepTime\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+\n",
      "| HR|  Qty|Ratio|\n",
      "+---+-----+-----+\n",
      "|  4|  127| 4.51|\n",
      "| 20|13339| 3.88|\n",
      "| 18|13034| 3.79|\n",
      "| 21|13628| 3.78|\n",
      "| 19|13782| 3.61|\n",
      "| 12|11324| 3.36|\n",
      "| 16|13131| 3.33|\n",
      "| 23| 6524| 3.32|\n",
      "|  0| 2381| 3.27|\n",
      "| 17|11308| 3.27|\n",
      "|  9| 9653|  3.2|\n",
      "| 22| 9444| 3.19|\n",
      "| 14|10837| 3.15|\n",
      "| 15|10205| 3.13|\n",
      "| 10|11291| 3.12|\n",
      "| 13|10160| 3.05|\n",
      "| 11|10755|  3.0|\n",
      "|  8| 8433| 2.98|\n",
      "|  7| 5316| 2.86|\n",
      "|  1|  474| 2.84|\n",
      "+---+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(sql_text.format(colname=\"CRSArrTime\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 截取部分資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Month', 'DayofMonth', 'DayOfWeek', 'UniqueCarrier', 'Origin', 'Dest']"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seleced_cols = [ ci[1] for ci in col_info if (ci[4]==\"O\")and(not \"Time\" in ci[1])and(not \"Distance\" in ci[1]) ]\n",
    "seleced_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = inspections.select(seleced_cols+[inspections.Cancelled.alias('label')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Month',\n",
       " 'DayofMonth',\n",
       " 'DayOfWeek',\n",
       " 'UniqueCarrier',\n",
       " 'Origin',\n",
       " 'Dest',\n",
       " 'label']"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
